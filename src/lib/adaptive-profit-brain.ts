/**
 * ADAPTIVE PROFIT MAXIMIZATION BRAIN V2.0
 * Unified self-learning mathematical consciousness that discovers optimal:
 * - Neural pathway weights (profit factors)
 * - Entry/exit thresholds (when to trade)
 * - Position sizing multipliers (how much to trade)
 * - Capital rotation timing (when to switch)
 *
 * Evolves through gradient descent with momentum, exploration-exploitation balance,
 * and continuous feedback from actual trade outcomes.
 *
 * NO HARDCODED THRESHOLDS - Pure mathematical learning
 */

interface TradeOutcome {
  symbol: string;
  expectedReturn: number;
  actualReturn: number;
  winProbability: number;
  actualWin: boolean;
  decisionFactors: {
    timeHeld: number;
    marketRegime: string;
    convictionLevel: number;
    opportunityCost: number;
    rotationScore: number;
  };
  profitImpact: number;
  timestamp: Date;
  // New: Decision type tracking for threshold learning
  decisionType?: 'entry' | 'hold' | 'exit' | 'skip';
  thresholdAtDecision?: number;
  confidenceLevel?: number;
}

interface NeuralPathway {
  factorName: string;
  weight: number;
  learningRate: number;
  momentum: number;
  lastGradient: number;
  activationHistory: number[];
  profitCorrelation: number;
}

// New: Learned threshold parameters
interface ThresholdParameter {
  name: string;
  currentValue: number;
  learningRate: number;
  momentum: number;
  velocity: number;
  minValue: number;
  maxValue: number;
  lastGradient: number;
  profitHistory: number[];
  adjustmentHistory: number[];
  explorationNoise: number;
  optimalEstimate: number; // Continuously updated estimate of optimal value
}

interface EquationGenome {
  id: string;
  formula: string;
  weights: Map<string, number>;
  fitness: number; // Profit per trade
  generation: number;
  mutationRate: number;
  parentIds: string[];
}

export class AdaptiveProfitBrain {
  private static instance: AdaptiveProfitBrain;
  private neuralPathways: Map<string, NeuralPathway> = new Map();
  private tradeHistory: TradeOutcome[] = [];
  private equationGenomes: EquationGenome[] = [];
  private currentGeneration: number = 0;
  private learningEnabled: boolean = true;

  // NEW: Self-learning thresholds
  private thresholds: Map<string, ThresholdParameter> = new Map();
  private maxHistoryLength = 1000;
  private performanceWindow = 50;
  private explorationDecayRate = 0.995;
  private minExplorationRate = 0.05;

  // Core mathematical factors that can be dynamically weighted
  private coreFacts = {
    'expectedReturn': 0.25,
    'winProbability': 0.20,
    'timeDecay': 0.15,
    'opportunityCost': 0.15,
    'convictionLevel': 0.10,
    'marketMomentum': 0.10,
    'transactionCost': -0.05
  };

  static getInstance(): AdaptiveProfitBrain {
    if (!AdaptiveProfitBrain.instance) {
      AdaptiveProfitBrain.instance = new AdaptiveProfitBrain();
    }
    return AdaptiveProfitBrain.instance;
  }

  private constructor() {
    this.initializeNeuralPathways();
    this.initializeThresholdParameters(); // NEW
    this.loadHistoricalOutcomes();
    console.log('üß† ADAPTIVE PROFIT BRAIN V2.0 initialized - Learning thresholds AND pathways');
  }

  /**
   * Initialize neural pathways from core mathematical factors
   */
  private initializeNeuralPathways(): void {
    for (const [factor, initialWeight] of Object.entries(this.coreFacts)) {
      this.neuralPathways.set(factor, {
        factorName: factor,
        weight: initialWeight,
        learningRate: 0.001 + (Math.random() * 0.009), // 0.001-0.01 range
        momentum: 0.9,
        lastGradient: 0,
        activationHistory: [],
        profitCorrelation: 0
      });
    }
  }

  /**
   * NEW: Initialize self-learning threshold parameters
   * These start with reasonable values but adapt through gradient descent
   */
  private initializeThresholdParameters(): void {
    const baseLearningRate = 0.001;
    const momentumDecay = 0.9;

    // Entry confidence threshold - üö® V3.11.1: AGGRESSIVE BOOTSTRAP for maximum opportunity capture
    this.thresholds.set('entryConfidence', {
      name: 'entryConfidence',
      currentValue: 0.05, // Start at 5% - let brain learn optimal from wins/losses
      learningRate: baseLearningRate * 2.0, // 2x faster learning
      momentum: momentumDecay,
      velocity: 0,
      minValue: 0.01, // As low as 1% to explore full opportunity space
      maxValue: 0.50, // Max 50%
      lastGradient: 0,
      profitHistory: [],
      adjustmentHistory: [],
      explorationNoise: 0.15, // Higher exploration
      optimalEstimate: 0.05
    });

    // Exit score threshold - Learn when to hold vs exit
    this.thresholds.set('exitScore', {
      name: 'exitScore',
      currentValue: 0.50, // Start at 50% - less conservative
      learningRate: baseLearningRate * 1.5,
      momentum: momentumDecay,
      velocity: 0,
      minValue: 0.10, // As low as 10% to hold winners longer
      maxValue: 0.95, // Max 95%
      lastGradient: 0,
      profitHistory: [],
      adjustmentHistory: [],
      explorationNoise: 0.12,
      optimalEstimate: 0.50
    });

    // Position sizing multiplier - Learn optimal trade sizes
    this.thresholds.set('positionSizeMultiplier', {
      name: 'positionSizeMultiplier',
      currentValue: 1.5, // Start at 1.5x - more aggressive
      learningRate: baseLearningRate * 1.0,
      momentum: momentumDecay,
      velocity: 0,
      minValue: 0.3, // Min 30% sizing
      maxValue: 3.0, // Max 3x for high-conviction trades
      lastGradient: 0,
      profitHistory: [],
      adjustmentHistory: [],
      explorationNoise: 0.10,
      optimalEstimate: 1.5
    });

    // Profit taking threshold - Learn optimal profit capture
    this.thresholds.set('profitTakingThreshold', {
      name: 'profitTakingThreshold',
      currentValue: 0.08, // Start at 8% - let brain learn from market
      learningRate: baseLearningRate * 2.5, // Fast learning for profit capture
      momentum: momentumDecay,
      velocity: 0,
      minValue: 0.02, // As low as 2% for quick scalps
      maxValue: 0.60, // Max 60% for big moves
      lastGradient: 0,
      profitHistory: [],
      adjustmentHistory: [],
      explorationNoise: 0.18, // High exploration for profit optimization
      optimalEstimate: 0.08
    });

    // Capital rotation urgency
    this.thresholds.set('capitalRotationUrgency', {
      name: 'capitalRotationUrgency',
      currentValue: 0.30,
      learningRate: baseLearningRate * 0.6,
      momentum: momentumDecay,
      velocity: 0,
      minValue: 0.10,
      maxValue: 0.80,
      lastGradient: 0,
      profitHistory: [],
      adjustmentHistory: [],
      explorationNoise: 0.10,
      optimalEstimate: 0.30
    });

    // Volatility adjustment factor
    this.thresholds.set('volatilityAdjustmentFactor', {
      name: 'volatilityAdjustmentFactor',
      currentValue: 1.0,
      learningRate: baseLearningRate * 0.4,
      momentum: momentumDecay,
      velocity: 0,
      minValue: 0.5,
      maxValue: 2.0,
      lastGradient: 0,
      profitHistory: [],
      adjustmentHistory: [],
      explorationNoise: 0.06,
      optimalEstimate: 1.0
    });

    console.log('üìä Initialized self-learning thresholds:');
    for (const [name, param] of this.thresholds) {
      console.log(`   ${name}: ${(param.currentValue * 100).toFixed(1)}% (range: ${(param.minValue * 100).toFixed(0)}-${(param.maxValue * 100).toFixed(0)}%)`);
    }
  }

  /**
   * Load recent trade outcomes from database for learning
   * Uses AdaptiveLearningPerformance for real historical data
   */
  private async loadHistoricalOutcomes(): Promise<void> {
    try {
      const { prisma } = await import('./prisma');

      // Load from AdaptiveLearningPerformance - real historical data with no fallbacks
      const adaptivePerformance = await prisma.adaptiveLearningPerformance.findMany({
        where: {
          totalSignals: { gte: 10 } // Only load pairs with meaningful history
        },
        orderBy: { updatedAt: 'desc' },
        take: 50
      });

      for (const perf of adaptivePerformance) {
        // Create synthetic trade outcome from aggregated performance data
        // This represents the average behavior of this symbol
        const outcome: TradeOutcome = {
          symbol: perf.symbol,
          expectedReturn: perf.avgPnL / Math.abs(perf.avgPnL || 1) * 10, // Estimate based on avg P&L
          actualReturn: perf.avgPnL,
          winProbability: perf.accuracy, // Real accuracy from database
          actualWin: perf.avgPnL > 0,
          decisionFactors: {
            timeHeld: 24, // Assume 24 hour average
            marketRegime: 'normal',
            convictionLevel: perf.confidence, // Real confidence from database
            opportunityCost: 0,
            rotationScore: perf.recentStreak / 10 // Convert streak to 0-1 score
          },
          profitImpact: perf.avgPnL,
          timestamp: perf.updatedAt
        };
        this.tradeHistory.push(outcome);
      }

      // No disconnect needed - using singleton
      console.log(`üìä Loaded ${this.tradeHistory.length} historical outcomes from AdaptiveLearningPerformance`);

      if (this.tradeHistory.length > 0) {
        this.evolveFromHistory();
      }
    } catch (error) {
      console.warn('‚ö†Ô∏è Could not load historical data for learning:', error.message);
      // Fail gracefully - brain will learn from live data only
    }
  }

  /**
   * Calculate holding hours from timestamp
   */
  private calculateHoldingHours(createdAt: Date): number {
    return Math.max((Date.now() - createdAt.getTime()) / (1000 * 60 * 60), 0.1);
  }

  /**
   * Evolve neural pathway weights based on historical outcomes
   */
  private evolveFromHistory(): void {
    if (this.tradeHistory.length < 5) return;

    console.log('üî¨ Evolving neural pathways from historical feedback...');

    // Calculate profit correlations for each factor
    for (const [factorName, pathway] of this.neuralPathways) {
      const correlations: number[] = [];

      for (const trade of this.tradeHistory) {
        let factorValue = 0;

        switch (factorName) {
          case 'expectedReturn':
            factorValue = trade.expectedReturn / 100;
            break;
          case 'winProbability':
            factorValue = trade.winProbability;
            break;
          case 'timeDecay':
            factorValue = Math.exp(-trade.decisionFactors.timeHeld / 24);
            break;
          case 'opportunityCost':
            factorValue = trade.decisionFactors.opportunityCost;
            break;
          case 'convictionLevel':
            factorValue = trade.decisionFactors.convictionLevel;
            break;
          case 'marketMomentum':
            factorValue = Math.tanh(trade.actualReturn / 10);
            break;
          case 'transactionCost':
            factorValue = -0.0084; // Always negative
            break;
        }

        const profitNormalized = Math.tanh(trade.profitImpact / 100); // Normalize to -1,1
        correlations.push(factorValue * profitNormalized);
      }

      // Calculate average correlation
      const avgCorrelation = correlations.reduce((a, b) => a + b, 0) / correlations.length;
      pathway.profitCorrelation = avgCorrelation;

      // Adaptive weight adjustment based on correlation strength
      const learningSignal = avgCorrelation * pathway.learningRate;
      const momentumTerm = pathway.momentum * pathway.lastGradient;
      const weightUpdate = learningSignal + momentumTerm;

      pathway.weight += weightUpdate;
      pathway.lastGradient = learningSignal;

      // Bounds checking
      pathway.weight = Math.max(-1.0, Math.min(1.0, pathway.weight));

      console.log(`  ${factorName}: weight ${pathway.weight.toFixed(4)} (correlation: ${avgCorrelation.toFixed(3)})`);
    }
  }

  /**
   * Get symbol-specific performance from database
   */
  private async getSymbolPerformance(symbol: string, side: 'long' | 'short' = 'long'): Promise<{
    accuracy: number;
    totalSignals: number;
    totalPnL: number;
    confidence: number;
    recentStreak: number;
  }> {
    try {
      const { prisma } = await import('./prisma');

      const performance = await prisma.adaptiveLearningPerformance.findUnique({
        where: {
          category_symbol: {
            category: side,
            symbol: symbol
          }
        }
      });

      // No disconnect needed - using singleton

      if (!performance || performance.totalSignals === 0) {
        return {
          accuracy: 0.5,
          totalSignals: 0,
          totalPnL: 0,
          confidence: 0.5,
          recentStreak: 0
        };
      }

      return {
        accuracy: performance.accuracy,
        totalSignals: performance.totalSignals,
        totalPnL: performance.totalPnL,
        confidence: performance.confidence,
        recentStreak: performance.recentStreak
      };
    } catch (error) {
      console.warn(`‚ö†Ô∏è Could not fetch performance for ${symbol}:`, error.message);
      return {
        accuracy: 0.5,
        totalSignals: 0,
        totalPnL: 0,
        confidence: 0.5,
        recentStreak: 0
      };
    }
  }

  /**
   * Calculate adaptive profit-maximizing decision using evolved neural pathways
   * This replaces static E = (W √ó A) - (L √ó B) with dynamic learning
   */
  async calculateAdaptiveProfitDecision(
    currentPosition: {
      symbol: string;
      actualReturn: number;
      expectedReturn: number;
      holdingDays: number;
      mathematicalProof: number;
      aiConfidence: number;
      currentValue: number;
    },
    newOpportunity?: {
      symbol: string;
      expectedReturn: number;
      winProbability: number;
      signalStrength: number;
      requiredCapital: number;
    }
  ): Promise<{
    action: 'hold' | 'rotate' | 'partial_exit' | 'full_exit';
    confidence: number;
    expectedProfit: number;
    neuralReasoning: string;
    pathwayActivations: Map<string, number>;
    symbolPerformance?: { accuracy: number; totalPnL: number; recentStreak: number };
  }> {

    // Get symbol-specific historical performance
    const symbolPerformance = await this.getSymbolPerformance(currentPosition.symbol);

    // Calculate factor activations for current situation
    const activations = new Map<string, number>();

    // Expected return activation
    const expectedReturnActivation = Math.tanh(currentPosition.expectedReturn / 20);
    activations.set('expectedReturn', expectedReturnActivation);

    // Win probability from AI confidence (boosted by symbol performance)
    let winProbActivation = Math.min(currentPosition.aiConfidence / 100, 1.0);

    // Apply symbol-specific performance adjustments
    if (symbolPerformance.totalSignals >= 50) {
      if (symbolPerformance.accuracy > 0.90) {
        // Excellent performer - boost confidence
        winProbActivation *= 1.5;
        console.log(`   üåü Symbol boost: ${currentPosition.symbol} has ${(symbolPerformance.accuracy * 100).toFixed(1)}% accuracy`);
      } else if (symbolPerformance.accuracy < 0.30) {
        // Poor performer - reduce confidence
        winProbActivation *= 0.3;
        console.log(`   ‚ö†Ô∏è Symbol penalty: ${currentPosition.symbol} has ${(symbolPerformance.accuracy * 100).toFixed(1)}% accuracy`);
      }
    }

    activations.set('winProbability', winProbActivation);

    // Time decay (positions become less attractive over time)
    const timeDecayActivation = Math.exp(-currentPosition.holdingDays / 7);
    activations.set('timeDecay', timeDecayActivation);

    // Opportunity cost (if better opportunity exists)
    let opportunityCostActivation = 0;
    if (newOpportunity) {
      const opportunityDiff = newOpportunity.expectedReturn - currentPosition.expectedReturn;
      opportunityCostActivation = Math.tanh(opportunityDiff / 10);
    }
    activations.set('opportunityCost', opportunityCostActivation);

    // Conviction level from mathematical proof (enhanced by symbol track record)
    let convictionActivation = currentPosition.mathematicalProof;

    // Boost conviction for proven winners
    if (symbolPerformance.totalSignals >= 50 && symbolPerformance.accuracy > 0.85) {
      convictionActivation *= 1.3;
    }

    activations.set('convictionLevel', convictionActivation);

    // Market momentum from recent performance
    const momentumActivation = Math.tanh(currentPosition.actualReturn / 15);
    activations.set('marketMomentum', momentumActivation);

    // Transaction cost penalty
    activations.set('transactionCost', -0.0084);

    // Calculate weighted decision score using evolved neural pathways
    let decisionScore = 0;
    let totalAbsWeight = 0;

    for (const [factorName, activation] of activations) {
      const pathway = this.neuralPathways.get(factorName);
      if (pathway) {
        const contribution = pathway.weight * activation;
        decisionScore += contribution;
        totalAbsWeight += Math.abs(pathway.weight);

        // Update pathway activation history for future learning
        pathway.activationHistory.push(activation);
        if (pathway.activationHistory.length > 50) {
          pathway.activationHistory.shift();
        }
      }
    }

    // Normalize decision score
    const normalizedScore = totalAbsWeight > 0 ? decisionScore / totalAbsWeight : 0;

    // Determine action based on evolved decision thresholds
    let action: 'hold' | 'rotate' | 'partial_exit' | 'full_exit';
    let confidence = Math.abs(normalizedScore);

    // Dynamic thresholds based on learning (symbol-specific)
    const rotateThreshold = await this.calculateDynamicThreshold('rotate', currentPosition.symbol);
    const exitThreshold = await this.calculateDynamicThreshold('exit', currentPosition.symbol);
    const partialExitThreshold = await this.calculateDynamicThreshold('partial', currentPosition.symbol);

    if (newOpportunity && normalizedScore > rotateThreshold) {
      action = 'rotate';
    } else if (normalizedScore < -exitThreshold) {
      action = 'full_exit';
    } else if (currentPosition.actualReturn > 20 && normalizedScore < partialExitThreshold) {
      action = 'partial_exit';
    } else {
      action = 'hold';
    }

    // Calculate expected profit based on action
    let expectedProfit = 0;
    if (action === 'rotate' && newOpportunity) {
      expectedProfit = (newOpportunity.expectedReturn * newOpportunity.winProbability / 100) - 0.84;
    } else if (action === 'hold') {
      expectedProfit = currentPosition.expectedReturn * winProbActivation / 100;
    } else if (action === 'partial_exit') {
      expectedProfit = currentPosition.actualReturn * 0.5; // Lock half
    } else {
      expectedProfit = currentPosition.actualReturn; // Full exit locks current return
    }

    // Generate reasoning from top pathway activations
    const topPathways = Array.from(this.neuralPathways.entries())
      .sort(([,a], [,b]) => Math.abs(b.weight) - Math.abs(a.weight))
      .slice(0, 3)
      .map(([name, pathway]) => `${name}: ${(pathway.weight * activations.get(name) * 100).toFixed(1)}%`)
      .join(', ');

    const neuralReasoning = `Neural decision score: ${normalizedScore.toFixed(3)} | ` +
                           `Top factors: ${topPathways} | ` +
                           `Symbol: ${(symbolPerformance.accuracy * 100).toFixed(1)}% acc, $${symbolPerformance.totalPnL.toFixed(0)} P&L | ` +
                           `Generation ${this.currentGeneration} learning active`;

    return {
      action,
      confidence,
      expectedProfit,
      neuralReasoning,
      pathwayActivations: activations,
      symbolPerformance: {
        accuracy: symbolPerformance.accuracy,
        totalPnL: symbolPerformance.totalPnL,
        recentStreak: symbolPerformance.recentStreak
      }
    };
  }

  /**
   * Calculate dynamic decision thresholds based on recent performance
   * Enhanced with symbol-specific adjustments
   */
  private async calculateDynamicThreshold(
    actionType: 'rotate' | 'exit' | 'partial',
    symbol?: string
  ): Promise<number> {
    const recentTrades = this.tradeHistory.slice(-20);
    if (recentTrades.length < 5) {
      // Default thresholds if not enough data
      return actionType === 'rotate' ? 0.3 : actionType === 'exit' ? -0.4 : -0.1;
    }

    const avgProfit = recentTrades.reduce((sum, t) => sum + t.profitImpact, 0) / recentTrades.length;
    const profitStd = Math.sqrt(
      recentTrades.reduce((sum, t) => sum + Math.pow(t.profitImpact - avgProfit, 2), 0) / recentTrades.length
    );

    // Adjust thresholds based on recent performance
    const performanceMultiplier = Math.tanh(avgProfit / Math.max(profitStd, 1));

    let baseThreshold: number;
    switch (actionType) {
      case 'rotate':
        baseThreshold = 0.3 - (performanceMultiplier * 0.15); // More aggressive if performing well
        break;
      case 'exit':
        baseThreshold = -0.4 + (performanceMultiplier * 0.2); // Less likely to exit if performing well
        break;
      case 'partial':
        baseThreshold = -0.1 + (performanceMultiplier * 0.1);
        break;
      default:
        baseThreshold = 0;
    }

    // Apply symbol-specific threshold adjustments
    if (symbol) {
      const symbolPerf = await this.getSymbolPerformance(symbol);

      if (symbolPerf.totalSignals >= 50) {
        if (symbolPerf.accuracy >= 0.90 && symbolPerf.totalPnL > 100) {
          // Excellent pair - more aggressive thresholds
          baseThreshold *= 0.7; // Make it easier to trade this symbol
          console.log(`   ‚≠ê Threshold adjusted for excellent pair ${symbol}: ${baseThreshold.toFixed(3)}`);
        } else if (symbolPerf.accuracy < 0.30) {
          // Poor pair - more conservative thresholds
          baseThreshold *= 1.5; // Make it harder to trade this symbol
          console.log(`   ‚ö†Ô∏è Threshold adjusted for poor pair ${symbol}: ${baseThreshold.toFixed(3)}`);
        }
      }
    }

    return baseThreshold;
  }

  /**
   * Record actual trade outcome for continuous learning
   * NOW: Also learns thresholds from outcomes
   */
  async recordTradeOutcome(outcome: TradeOutcome): Promise<void> {
    this.tradeHistory.push(outcome);

    // Keep history manageable
    if (this.tradeHistory.length > this.maxHistoryLength) {
      this.tradeHistory.shift();
    }

    // Trigger adaptive learning every 5 trades
    if (this.tradeHistory.length % 5 === 0) {
      this.adaptFromRecentOutcome(outcome);
    }

    // NEW: Learn thresholds from this outcome
    if (outcome.decisionType && outcome.thresholdAtDecision !== undefined) {
      await this.updateThresholdsFromOutcome(outcome);
    }

    // Decay exploration rate
    for (const param of this.thresholds.values()) {
      param.explorationNoise = Math.max(
        this.minExplorationRate,
        param.explorationNoise * this.explorationDecayRate
      );
    }

    console.log(`üìà Recorded outcome: ${outcome.symbol} expected ${outcome.expectedReturn.toFixed(1)}% got ${outcome.actualReturn.toFixed(1)}% (profit: $${outcome.profitImpact.toFixed(2)})`);
  }

  /**
   * Immediate adaptation from single trade outcome
   */
  private adaptFromRecentOutcome(outcome: TradeOutcome): void {
    if (!this.learningEnabled) return;

    const profitError = outcome.actualReturn - outcome.expectedReturn;
    const learningSignal = Math.tanh(profitError / 10); // Normalize error

    // Adjust pathway weights based on outcome
    for (const [factorName, pathway] of this.neuralPathways) {
      let factorContribution = 0;

      switch (factorName) {
        case 'expectedReturn':
          factorContribution = outcome.expectedReturn / 100;
          break;
        case 'winProbability':
          factorContribution = outcome.actualWin ? 1 : -1;
          break;
        case 'timeDecay':
          factorContribution = Math.exp(-outcome.decisionFactors.timeHeld / 24);
          break;
      }

      // Update weight based on factor's contribution to this outcome
      const weightAdjustment = learningSignal * factorContribution * pathway.learningRate;
      pathway.weight += weightAdjustment;
      pathway.weight = Math.max(-1, Math.min(1, pathway.weight));
    }

    console.log(`üß† Neural pathways adapted from ${outcome.symbol} outcome (error: ${profitError.toFixed(1)}%)`);
  }

  /**
   * Get current pathway state for debugging/monitoring
   */
  getPathwayState(): Array<{factorName: string; weight: number; correlation: number}> {
    return Array.from(this.neuralPathways.entries()).map(([name, pathway]) => ({
      factorName: name,
      weight: pathway.weight,
      correlation: pathway.profitCorrelation
    }));
  }

  /**
   * Enable/disable learning (for testing)
   */
  setLearningEnabled(enabled: boolean): void {
    this.learningEnabled = enabled;
    console.log(`üß† Adaptive learning ${enabled ? 'ENABLED' : 'DISABLED'}`);
  }

  // ============================================================================
  // NEW: SELF-LEARNING THRESHOLD METHODS
  // ============================================================================

  /**
   * Get threshold value with contextual adjustments and exploration
   */
  getThreshold(
    parameterName: string,
    context?: {
      volatility?: number;
      regime?: string;
      confidence?: number;
      marketMomentum?: number;
    }
  ): number {
    const param = this.thresholds.get(parameterName);
    if (!param) {
      console.warn(`‚ö†Ô∏è Unknown threshold parameter: ${parameterName}`);
      return 0.15;
    }

    // Epsilon-greedy exploration
    const shouldExplore = Math.random() < param.explorationNoise;

    let value = param.currentValue;

    // Apply contextual adjustments
    if (context) {
      if (context.volatility !== undefined && parameterName === 'entryConfidence') {
        const volatilityFactor = this.thresholds.get('volatilityAdjustmentFactor')?.currentValue ?? 1.0;
        value += context.volatility * volatilityFactor * 0.5;
      }

      if (context.regime === 'BULL' && parameterName === 'entryConfidence') {
        value *= 0.9;
      } else if (context.regime === 'BEAR' && parameterName === 'entryConfidence') {
        value *= 1.1;
      }

      if (context.confidence !== undefined && parameterName === 'positionSizeMultiplier') {
        value *= (0.8 + context.confidence * 0.4);
      }
    }

    // Exploration noise
    if (shouldExplore) {
      const noiseRange = (param.maxValue - param.minValue) * 0.1;
      const noise = (Math.random() - 0.5) * 2 * noiseRange;
      value += noise;
    }

    return Math.max(param.minValue, Math.min(param.maxValue, value));
  }

  /**
   * Update thresholds using gradient descent based on outcome
   */
  private async updateThresholdsFromOutcome(outcome: TradeOutcome): Promise<void> {
    let thresholdName: string;

    switch (outcome.decisionType) {
      case 'entry':
        thresholdName = 'entryConfidence';
        break;
      case 'exit':
        thresholdName = 'exitScore';
        break;
      case 'hold':
        thresholdName = 'exitScore';
        break;
      case 'skip':
        thresholdName = 'entryConfidence';
        break;
      default:
        return;
    }

    const param = this.thresholds.get(thresholdName);
    if (!param) return;

    // Calculate gradient
    const gradient = this.calculateProfitGradient(
      thresholdName,
      outcome.profitImpact,
      outcome.confidenceLevel ?? 0.5,
      outcome.thresholdAtDecision ?? param.currentValue
    );

    // Update with momentum
    param.velocity = param.momentum * param.velocity + param.learningRate * gradient;

    const oldValue = param.currentValue;
    param.currentValue += param.velocity;
    param.currentValue = Math.max(param.minValue, Math.min(param.maxValue, param.currentValue));

    // Track history
    param.lastGradient = gradient;
    param.profitHistory.push(outcome.profitImpact);
    param.adjustmentHistory.push(param.currentValue - oldValue);

    if (param.profitHistory.length > 100) {
      param.profitHistory.shift();
      param.adjustmentHistory.shift();
    }

    // Update optimal estimate
    param.optimalEstimate = this.estimateOptimalThreshold(thresholdName);

    // Adjust learning rate based on variance
    if (param.profitHistory.length > 20) {
      const recentVariance = this.calculateVariance(param.profitHistory.slice(-20));
      param.learningRate = 0.001 / (1 + recentVariance * 0.01);
    }

    const percentChange = ((param.currentValue - oldValue) / oldValue) * 100;
    if (Math.abs(percentChange) > 1) {
      console.log(`üß† THRESHOLD LEARNED: ${thresholdName} ${percentChange > 0 ? '+' : ''}${percentChange.toFixed(2)}%`);
      console.log(`   ${(oldValue * 100).toFixed(2)}% ‚Üí ${(param.currentValue * 100).toFixed(2)}% (optimal est: ${(param.optimalEstimate * 100).toFixed(2)}%)`);
      console.log(`   Gradient: ${gradient.toFixed(4)}, Profit: $${outcome.profitImpact.toFixed(2)}`);
    }
  }

  /**
   * Calculate profit gradient with respect to threshold
   */
  private calculateProfitGradient(
    thresholdName: string,
    actualProfit: number,
    confidenceLevel: number,
    thresholdAtDecision: number
  ): number {
    const recentDecisions = this.tradeHistory.slice(-this.performanceWindow);

    if (recentDecisions.length < 10) {
      return actualProfit > 0 ? 0.01 : -0.01;
    }

    const avgProfit = recentDecisions.reduce((sum, d) => sum + d.profitImpact, 0) / recentDecisions.length;
    const normalizedProfit = (actualProfit - avgProfit) / (Math.abs(avgProfit) + 1);

    let gradient = 0;

    if (thresholdName === 'entryConfidence') {
      if (actualProfit > 0) {
        if (confidenceLevel > thresholdAtDecision + 0.05) {
          gradient = -0.01 * normalizedProfit; // Can lower threshold
        } else {
          gradient = 0.005 * normalizedProfit; // Threshold is good
        }
      } else {
        if (confidenceLevel < thresholdAtDecision + 0.05) {
          gradient = 0.02 * Math.abs(normalizedProfit); // Raise threshold
        } else {
          gradient = 0.001;
        }
      }
    } else if (thresholdName === 'exitScore') {
      if (actualProfit > avgProfit * 1.5) {
        gradient = -0.01; // Could exit earlier
      } else if (actualProfit < avgProfit * 0.5) {
        gradient = 0.01; // Exited too early
      } else {
        gradient = 0.001 * normalizedProfit;
      }
    }

    return gradient;
  }

  /**
   * Estimate optimal threshold from profitable decisions
   */
  private estimateOptimalThreshold(thresholdName: string): number {
    const param = this.thresholds.get(thresholdName);
    if (!param || param.profitHistory.length < 10) {
      return param?.currentValue ?? 0.15;
    }

    const recentAdjustments = param.adjustmentHistory.slice(-20);
    const recentProfits = param.profitHistory.slice(-20);

    let sumProfitWeightedThreshold = 0;
    let sumProfitWeights = 0;
    let currentThreshold = param.currentValue;

    for (let i = recentAdjustments.length - 1; i >= 0; i--) {
      currentThreshold -= recentAdjustments[i];
      const profit = recentProfits[i];

      if (profit > 0) {
        sumProfitWeightedThreshold += currentThreshold * profit;
        sumProfitWeights += profit;
      }
    }

    return sumProfitWeights > 0 ? sumProfitWeightedThreshold / sumProfitWeights : param.currentValue;
  }

  /**
   * Calculate variance
   */
  private calculateVariance(values: number[]): number {
    if (values.length === 0) return 0;
    const mean = values.reduce((sum, v) => sum + v, 0) / values.length;
    return values.reduce((sum, v) => sum + Math.pow(v - mean, 2), 0) / values.length;
  }

  /**
   * Get current learned thresholds
   */
  getCurrentThresholds(): Record<string, {
    current: number;
    optimal: number;
    exploration: number;
    profitHistory: number;
  }> {
    const thresholds: Record<string, any> = {};
    for (const [name, param] of this.thresholds) {
      const avgProfit = param.profitHistory.length > 0
        ? param.profitHistory.reduce((sum, p) => sum + p, 0) / param.profitHistory.length
        : 0;

      thresholds[name] = {
        current: param.currentValue,
        optimal: param.optimalEstimate,
        exploration: param.explorationNoise,
        profitHistory: avgProfit
      };
    }
    return thresholds;
  }

  /**
   * Get learning performance metrics
   */
  getLearningMetrics(): {
    pathways: Array<{factorName: string; weight: number; correlation: number}>;
    thresholds: Record<string, {
      value: number;
      optimalEstimate: number;
      avgProfit: number;
      decisions: number;
      convergence: number;
    }>;
    totalDecisions: number;
  } {
    const pathways = this.getPathwayState();
    const thresholds: Record<string, any> = {};
    let totalDecisions = 0;

    for (const [name, param] of this.thresholds) {
      const avgProfit = param.profitHistory.length > 0
        ? param.profitHistory.reduce((sum, p) => sum + p, 0) / param.profitHistory.length
        : 0;

      // Convergence: how close current is to optimal estimate
      const convergence = param.optimalEstimate > 0
        ? 1 - Math.abs(param.currentValue - param.optimalEstimate) / param.optimalEstimate
        : 0;

      const decisions = param.profitHistory.length;
      totalDecisions = Math.max(totalDecisions, decisions); // Use max across all thresholds

      thresholds[name] = {
        value: param.currentValue,
        optimalEstimate: param.optimalEstimate,
        avgProfit,
        decisions,
        convergence: Math.max(0, Math.min(1, convergence))
      };
    }

    return { pathways, thresholds, totalDecisions };
  }
}

export const adaptiveProfitBrain = AdaptiveProfitBrain.getInstance();